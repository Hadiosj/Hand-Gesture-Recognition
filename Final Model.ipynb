{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import cv2\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from keras import models, layers, optimizers\n",
    "from keras.applications import VGG16\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image as image_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "style.use('seaborn-whitegrid')\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gestures = {'L_': 'L',\n",
    "           'fi': 'Fist',\n",
    "           'C_': 'C',\n",
    "           'ok': 'Okay',\n",
    "           'pe': 'Peace',\n",
    "           'pa': 'Palm'\n",
    "            }\n",
    "\n",
    "gestures_map = {'Fist' : 0,\n",
    "                'L': 1,\n",
    "                'Okay': 2,\n",
    "                'Palm': 3,\n",
    "                'Peace': 4\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(path):\n",
    "    img = Image.open(path)\n",
    "    img = img.resize((224, 224))\n",
    "    img = np.array(img)\n",
    "    return img\n",
    "\n",
    "def process_data(X_data, y_data):\n",
    "    X_data = np.array(X_data, dtype = 'float32')\n",
    "    if rgb:\n",
    "        pass\n",
    "    else:\n",
    "        X_data = np.stack((X_data,)*3, axis=-1)\n",
    "    X_data /= 255\n",
    "    y_data = np.array(y_data)\n",
    "    y_data = to_categorical(y_data)\n",
    "    return X_data, y_data\n",
    "\n",
    "def walk_file_tree(relative_path):\n",
    "    X_data = []\n",
    "    y_data = [] \n",
    "    for directory, subdirectories, files in os.walk(relative_path):\n",
    "        for file in files:\n",
    "            if not file.startswith('.') and (not file.startswith('C_')):\n",
    "                path = os.path.join(directory, file)\n",
    "                gesture_name = gestures[file[0:2]]\n",
    "                y_data.append(gestures_map[gesture_name])\n",
    "                X_data.append(process_image(path))   \n",
    "\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    X_data, y_data = process_data(X_data, y_data)\n",
    "    return X_data, y_data\n",
    "\n",
    "class Data(object):\n",
    "    def __init__(self):\n",
    "        self.X_data = []\n",
    "        self.y_data = []\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.X_data, self.y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get 'silhouette' data (this is the data we used for the final model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path = './frames/silhouettes/'\n",
    "rgb = False\n",
    "\n",
    "# # This method processes the data\n",
    "X_data, y_data = walk_file_tree(relative_path)\n",
    "\n",
    "# Can also optionally use a class to get this data, in order to keep it separate from Drawing data\n",
    "silhouette = Data()\n",
    "silhouette.X_data, silhouette.y_data = walk_file_tree(relative_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X_data shape: {X_data.shape}')\n",
    "print(f'y_data shape: {y_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get 'drawing' data (this was not used for final model, but I built and tested several models with it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "relative_path = './frames/drawings/'\n",
    "rgb = True\n",
    "\n",
    "# This method processes the data\n",
    "X_data, y_data = walk_file_tree(relative_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'X_data shape: {X_data.shape}')\n",
    "print(f'y_data shape: {y_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the dictionaries to a dataframe to be saved for future use\n",
    "# d = {'image_path':image_path, 'gesture':gesture, 'image_rgb': image_rgb, 'image_bw_x': X_data, 'image_bw_y': y_data}\n",
    "d = {'image_path':image_path, 'gesture':gesture}\n",
    "df = pd.DataFrame(d)\n",
    "# df['gesture_num'] = df['gesture'].apply(lambda x: x[1:2])\n",
    "# df['gesture_name'] = df['gesture'].apply(lambda x: x[3:])\n",
    "\n",
    "# df.to_csv('silhouette_df.csv')\n",
    "# df = pd.read_csv('silhouette_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bring in Kaggle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gestures_map = {3: 0,\n",
    "                2: 1,\n",
    "                7: 2,\n",
    "                1: 3,\n",
    "                'Peace': 4\n",
    "                }\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "root_dir = os.fsencode('./data/gestures_data/')\n",
    "\n",
    "for directory, subdirectories, files in os.walk(root_dir):\n",
    "    for file in files:\n",
    "        if not file.startswith(b'.'):\n",
    "            gesture_name = int(file.decode('utf8')[10:11])\n",
    "            if gesture_name in [1, 2, 3, 7]:\n",
    "                path = os.path.join(directory, file).decode('utf8')\n",
    "                y_data.append(gestures_map[gesture_name])\n",
    "\n",
    "                img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "                img = cv2.flip(img, 1)\n",
    "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                blur = cv2.GaussianBlur(gray, (41, 41), 0)  #tuple indicates blur value\n",
    "                ret, thresh = cv2.threshold(blur, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "                thresh = cv2.resize(thresh, (224, 224))\n",
    "                thresh = np.array(thresh)\n",
    "                X_data.append(thresh)\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "process_data(X_data, y_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to add a column of zeroes for the 'Peace' sign, since the Kaggle data does not have photos of \n",
    "# 'Peace' signs.\n",
    "z = np.zeros(len(y_data))\n",
    "y_data = np.append(y_data, z, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice that the Kaggle images are now thresholded and binarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(.5 - X_data[250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(.5 - X_data[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(.5 - X_data[1200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_rgb, X_test_rgb, y_train_rgb, y_test_rgb = train_test_split(image_rgb, y_data, test_size = 0.2, random_state=12, stratify=y_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.2, random_state=12, stratify=y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the VGG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './models/saved_model.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(filepath=file_path, save_best_only=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy',\n",
    "                               min_delta=0,\n",
    "                               patience=10,\n",
    "                               verbose=1,\n",
    "                               mode='auto',\n",
    "                               restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageSize=224\n",
    "vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=(imageSize, imageSize, 3))\n",
    "#vgg_base = VGG16(weights='imagenet', include_top=True)\n",
    "optimizer1 = optimizers.Adam()\n",
    "\n",
    "base_model = vgg_base  # Topless\n",
    "# Add top layer\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu', name='fc1')(x)\n",
    "x = Dense(128, activation='relu', name='fc2')(x)\n",
    "x = Dense(128, activation='relu', name='fc3')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='relu', name='fc4')(x)\n",
    "predictions = Dense(5, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Train top layers only\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_train, y_train), verbose=1,\n",
    "          callbacks=[early_stopping, model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load VGG16\n",
    "# Get back the convolutional part of a VGG network trained on ImageNet\n",
    "\n",
    "imageSize = 224\n",
    "model1 = VGG16(weights='imagenet', include_top=False, input_shape=(imageSize, imageSize, 3))\n",
    "optimizer1 = optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "base_model = model1  # Topless\n",
    "# Add top layer\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu', name='fc1')(x)\n",
    "x = Dense(128, activation='relu', name='fc2')(x)\n",
    "x = Dense(128, activation='relu', name='fc3')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='relu', name='fc4')(x)\n",
    "\n",
    "predictions = Dense(5, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Train top layer\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1)]\n",
    "callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=64, validation_data=(X_train, y_train), verbose=1,\n",
    "          callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "'''\n",
    "# Uncomment the section below and use in lieu of model.fit() above\n",
    "# if using image augmentation. Our final model did not.\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=45.,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "fits the model on batches with real-time data augmentation:\n",
    "model.fit_generator(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                    steps_per_epoch=len(X_train)/32, epochs=150, validation_data=(X_test, y_test))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save and import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('models/VGG_reversed.h5')\n",
    "\n",
    "from keras.models import load_model\n",
    "model = load_model('/home/ubuntu/project_kojak/models/VGG_reversed.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_metrics(X_test, y_test):\n",
    "    pred = model.predict(X_test)\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    print(confusion_matrix(y_true, pred))\n",
    "    print('\\n')\n",
    "    print(classification_report(y_true, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VGG_cross_validated model**\n",
    "\n",
    "Note that there are no predictions for class \\#5 since 'Peace' sign did not exist in cross_val images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_classification_metrics(X_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict gesture on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture_names = {0: 'C',\n",
    "                 1: 'Fist',\n",
    "                 2: 'L',\n",
    "                 3: 'Okay',\n",
    "                 4: 'Palm',\n",
    "                 5: 'Peace'}\n",
    "\n",
    "def predict_rgb_image(path):\n",
    "    img2rgb = image_utils.load_img(path=path, target_size=(224, 224))\n",
    "    img2rgb = image_utils.img_to_array(img2rgb)\n",
    "    img2rgb = img2rgb.reshape(1, 224, 224, 3)\n",
    "    return gesture_names[np.argmax(model.predict(img2rgb))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_rgb_image('images_to_predict/test - palm.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=45.,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "datagen.fit(X_train_rgb)\n",
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "model.fit_generator(datagen.flow(X_train_rgb, y_train_rgb, batch_size=32),\n",
    "                    steps_per_epoch=len(X_train_rgb) / 128, epochs=10, validation_data=(X_test_rgb, y_test_rgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(model.predict(X_test_rgb[0].reshape(1, 224, 224, 3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build own model\n",
    "Note that we ended up using the model on top of VGG, not this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25, seed=21))\n",
    "model.add(layers.Dense(5, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train[:500], y_train[:500], epochs=4, batch_size=64, validation_data=(X_test[:500], y_test[:500]), verbose=1, callbacks = [MetricsCheckpoint('logs')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_rgb, y_train_rgb, epochs=200, batch_size=16, validation_data=(X_test_rgb, y_test_rgb), verbose=1, callbacks =[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_reports(y_pred, y_true):\n",
    "    y_pred_classes = np.array(np.argmax(y_pred))  # reconverts back from one hot encoded \n",
    "    y_true = np.array(np.argmax(y_true))  # reconverts back from one hot encoded\n",
    "    print(confusion_matrix(y_true, y_pred_classes))\n",
    "    print(classification_report(y_true, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification metrics for an earlier version of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_pred_classes, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_pred_classes, y_true))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
